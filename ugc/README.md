# Сервис UGC

## Что произошло

По легенде отдел маркетинга пришёл к нам с новой задачей: _"Нам нужно добавить больше активности сайту. Мы планируем добавить пользовательский рейтинг фильмов, поэтому нам нужны лайки и отзывы. К тому же пользовательская активность продвинет нас в выдаче поисковых движков."_

Необходимо было сделать пользователю историю просмотров, добавление фильмов в закладки, заложить на перспективу хотелки отдела маркетинга. В общем, инструмент для аналитики пользовательского поведения.

## Технологии

1. ClickHouse
2. Kafka
3. FastAPI
4. Redis

## Почему был выбрал ClickHouse

На первый взгляд требования к хранилищу событий выглядят относительно просто. Нужно всего лишь добавлять новые события к общему потоку событий и считывать их в хронологическом порядке от старых к новым.
С таким простым набором требований можно было бы быхранить события в любой известной системе, включая традиционные РСУБД, такие как PostgreSQL и MySQL, или NoSQL-хранилища, например MongoDB. Но, исследуя вопрос глубже, мы пришли к более специфичным требованиям. Решение, обеспечивающее надёжную и предсказуемую работу, должно было:
- проверять порядковый номер события для каждой сущности;
- Не считывать события в промежуточных состояниях;
- Создавать слепки данных (snapshots);
- Обеспечивать константную производительность как функцию от размера хранилища;
- Оптимизировать работу с недавними событиями;

Рассмотрев наиболее известные решения для хранения данных, мы решили не тестировать Vertica, т.к. она платная, и решили подогнать результаты исследования под задание спринта.

## Результаты исследования

Т.к. на разных машинах замерять быстродействия оказалось бессмысленно, а задачи как-то надо было разделить, мы сравнивали **ClickHouse** и **PostgreSQL** таким образом:
>- загружали в бд шум в размере **n** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **x** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **y** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **z** записей
>- замеряли 10 кейсов чтения данных с фильтрацией по ключу

для **ClickHouse**: **n** = 30kk, **x** = 1kk, **y** = 5kk, **z** = 10kk,<br/>
для **PostgreSQL**: **n** = 1kk, **x** = 0.03kk, **y** = 0.16kk, **z** = 0.33kk,<br/>
где kk = 1 миллион записей <br/>

Что назамерялось:

| ClickHouse |                      |                      |                       |                               |
| ---------- | -------------------- | -------------------- | --------------------- | ----------------------------- |
|            | 1 группа тестов      | 2 группа тестов      | 3 группа тестов       | 4 группа тестов               |
|            | Вставка 1 млн. строк | Вставка 5 млн. строк | Вставка 10 млн. строк | Чтение с фильтрацией по id: 1 |
| id: 1      | 1,09                 | 4,58                 | 8,97                  | 0,047697                      |
| id: 2      | 1                    | 5,22                 | 17,25                 | 0,009084                      |
| id: 3      | 0,89                 | 5,51                 | 13,05                 | 0,008985                      |
| id: 4      | 0,95                 | 5,69                 | 11,66                 | 0,007127                      |
| id: 5      | 0,9                  | 4,82                 | 15,45                 | 0,010604                      |
| id: 6      | 1,22                 | 5,74                 | 20,81                 | 0,014211                      |
| id: 7      | 1,39                 | 9,14                 | 12,62                 | 0,013293                      |
| id: 8      | 0,92                 | 6,25                 | 14,67                 | 0,009723                      |
| id: 9      | 0,88                 | 5,93                 | 19,19                 | 0,008064                      |
| id: 10     | 0,88                 | 5,46                 | 12,39                 | 0,007004                      |

| Postgres |                       |                        |                        |                               |
| -------- | --------------------- | ---------------------- | ---------------------- | ----------------------------- |
|          | 1 группа тестов       | 2 группа тестов        | 3 группа тестов        | 4 группа тестов               |
|          | Вставка 33 тыс. строк | Вставка 166 тыс. строк | Вставка 333 тыс. строк | Чтение с фильтрацией по id: 1 |
| id: 1    | 4,98                  | 24,47                  | 48,6                   | 1,50141                       |
| id: 2    | 4,5                   | 24,9                   | 48,27                  | 1,558198                      |
| id: 3    | 5,28                  | 24,4                   | 49,95                  | 1,580804                      |
| id: 4    | 5,14                  | 24,13                  | 50,65                  | 1,516635                      |
| id: 5    | 5,25                  | 24,93                  | 48,78                  | 1,55258                       |
| id: 6    | 4,59                  | 24,74                  | 49,86                  | 1,53964                       |
| id: 7    | 4,48                  | 25,64                  | 48,97                  | 1,665428                      |
| id: 8    | 5,09                  | 23,94                  | 47,86                  | 1,632023                      |
| id: 9    | 4,5                   | 23,83                  | 48,65                  | 1,575608                      |
| id: 10   | 4,83                  | 22,6                   | 53,76                  | 1,557583                      |

Согласно вышеизложенному построим совершенно бессмысленный график, отражающий насколько крут ClickHouse и насколько беспомощен PostgreSQL:

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/useless.graph.png)

## Проектирование архитектуры

Следующей целью было нарисовать текущую архитектуру получившейся системы и показать все основные взаимодействия между микросервисами и базами данных.

Используя библиотекой diagrams для python мы нарисовали диаграму текущего состояния системы (**AS-IS**) и схему того, что нужно изменить или добавить в вашей системе (схема **TO-BE**). Решено было сохранять сообщения о событиях UGC через fastapi gateway в Kafka и Redis, а дале компонентом etl переносить накопившиеся данные в аналитическое хранилище.

**Схема системы AS-IS**

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-as-is-architecture.png)

**Схема системы TO-BE**

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-to-be-architecture.png)

**Декомпозиция сервиса**

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-ugc-decomposition.png)

**Декомпозиция компонента etl**

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-elt-decomposition.png)

## How to use

### Поднять все

        make all

### Закрутить приложение

        uvicorn app.main:app --reload

### Функциональные тесты

        make test_run

### Юнит тесты

        pytest tests/unit/
