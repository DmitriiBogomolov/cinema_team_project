# Сервис UGC

## Что произошло

По легенде отдел маркетинга пришёл к нам с новой задачей: _"Нам нужно добавить больше активности сайту. Мы планируем добавить пользовательский рейтинг фильмов, поэтому нам нужны лайки и отзывы. К тому же пользовательская активность продвинет нас в выдаче поисковых движков."_

Необходимо было сделать пользователю историю просмотров, добавление фильмов в закладки, заложить на перспективу хотелки отдела маркетинга. В общем, инструмент для аналитики пользовательского поведения.

## Технологии

1. ClickHouse
2. Kafka
3. FastAPI
4. Redis

## Выбор технологий

На первый взгляд требования к хранилищу событий выглядят относительно просто. Нужно всего лишь добавлять новые события к общему потоку событий и считывать их в хронологическом порядке от старых к новым.
С таким простым набором требований можно было бы быхранить события в любой известной системе, включая традиционные РСУБД, такие как PostgreSQL и MySQL, или NoSQL-хранилища, например MongoDB. Но, исследуя вопрос глубже, приходим к более специфичным требованиям. Решение, обеспечивающее надёжную и предсказуемую работу, должно было:
- проверять порядковый номер события для каждой сущности;
- Не считывать события в промежуточных состояниях;
- Создавать слепки данных (snapshots);
- Обеспечивать константную производительность как функцию от размера хранилища;
- Оптимизировать работу с недавними событиями;

Рассмотрев наиболее известные решения для хранения данных, мы решили не тестировать Vertica, т.к. она платная, и решили подогнать результаты исследования под задание спринта.

## Исследование **ClickHouse** и **PostgreSQL**

**ClickHouse** и **PostgreSQL** сравнивали таким образом:
>- загружали в бд шум в размере **n** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **x** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **y** записей
>- замеряли 10 кейсов загрузки данных в бд батчем по **z** записей
>- замеряли 10 кейсов чтения данных с фильтрацией по ключу

для **ClickHouse**: **n** = 30kk, **x** = 1kk, **y** = 5kk, **z** = 10kk,<br/>
для **PostgreSQL**: **n** = 1kk, **x** = 0.03kk, **y** = 0.16kk, **z** = 0.33kk,<br/>
где kk = 1 миллион записей <br/>

Что замерялось:

| ClickHouse |                      |                      |                       |                               |
| ---------- | -------------------- | -------------------- | --------------------- | ----------------------------- |
|            | 1 группа тестов      | 2 группа тестов      | 3 группа тестов       | 4 группа тестов               |
|            | Вставка 1 млн. строк | Вставка 5 млн. строк | Вставка 10 млн. строк | Чтение с фильтрацией по id: 1 |
| id: 1      | 1,09                 | 4,58                 | 8,97                  | 0,047697                      |
| id: 2      | 1                    | 5,22                 | 17,25                 | 0,009084                      |
| id: 3      | 0,89                 | 5,51                 | 13,05                 | 0,008985                      |
| id: 4      | 0,95                 | 5,69                 | 11,66                 | 0,007127                      |
| id: 5      | 0,9                  | 4,82                 | 15,45                 | 0,010604                      |
| id: 6      | 1,22                 | 5,74                 | 20,81                 | 0,014211                      |
| id: 7      | 1,39                 | 9,14                 | 12,62                 | 0,013293                      |
| id: 8      | 0,92                 | 6,25                 | 14,67                 | 0,009723                      |
| id: 9      | 0,88                 | 5,93                 | 19,19                 | 0,008064                      |
| id: 10     | 0,88                 | 5,46                 | 12,39                 | 0,007004                      |

| Postgres |                       |                        |                        |                               |
| -------- | --------------------- | ---------------------- | ---------------------- | ----------------------------- |
|          | 1 группа тестов       | 2 группа тестов        | 3 группа тестов        | 4 группа тестов               |
|          | Вставка 33 тыс. строк | Вставка 166 тыс. строк | Вставка 333 тыс. строк | Чтение с фильтрацией по id: 1 |
| id: 1    | 4,98                  | 24,47                  | 48,6                   | 1,50141                       |
| id: 2    | 4,5                   | 24,9                   | 48,27                  | 1,558198                      |
| id: 3    | 5,28                  | 24,4                   | 49,95                  | 1,580804                      |
| id: 4    | 5,14                  | 24,13                  | 50,65                  | 1,516635                      |
| id: 5    | 5,25                  | 24,93                  | 48,78                  | 1,55258                       |
| id: 6    | 4,59                  | 24,74                  | 49,86                  | 1,53964                       |
| id: 7    | 4,48                  | 25,64                  | 48,97                  | 1,665428                      |
| id: 8    | 5,09                  | 23,94                  | 47,86                  | 1,632023                      |
| id: 9    | 4,5                   | 23,83                  | 48,65                  | 1,575608                      |
| id: 10   | 4,83                  | 22,6                   | 53,76                  | 1,557583                      |

Для поставленной PostgreSQL явно не подходит, а в качестве аналитического хранилища был выбран ClickHouse:

![alt text](https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/useless.graph.png)


## Исследование **MongoDB** и **PostgreSQL**

Далее необходимо было выбрать хранилище для оставшихся данных UGC (оценки, рецензии к фильмам, пользовательские закладки). В качестве СУБД для исследования были выбраны **MongoDB** и **PostgreSQL**.

Как загружали данные:
>- загружали в бд шум в размере 100 миллионов записей/документов
>- загружали в бд тестовые данные:
>- фильм с 10 000 оценок
>- пользователя, поставившего 1000 лайков
>- пользователя с 1000 закладок

Как сравнивали:
>- Первый тест: запрос количества оценок фильма
>- Второй тест: запрос средней оценки фильма
>- Третий тест: запрос списка оценок пользователя
>- Четвертый тест: запрос списка закладок пользователя
>- Пятый тест: добавление оценки фильму

Что получилось:
| MongoDB     |             |             |             | Время указано в секундах |            |
| ----------- | ----------- | ----------- | ----------- | ------------------------ | ---------- |
|             | Первый тест | Второй тест | Третий тест | Четвертый тест           | Пятый тест |
| 1 итерация  | 0,019       | 0,004       | 10,507      | 0,321                    | 0,035      |
| 2 итерация  | 0,003       | 0,002       | 8,303       | 0,258                    | 0,022      |
| 3 итерация  | 0,004       | 0,002       | 8,672       | 0,253                    | 0,012      |
| 4 итерация  | 0,004       | 0,003       | 11,545      | 0,262                    | 0,015      |
| 5 итерация  | 0,004       | 0,002       | 11,464      | 0,251                    | 0,015      |
| 6 итерация  | 0,004       | 0,002       | 11,198      | 0,251                    | 0,012      |
| 7 итерация  | 0,004       | 0,002       | 9,581       | 0,256                    | 0,012      |
| 8 итерация  | 0,004       | 0,001       | 13,916      | 0,246                    | 0,012      |
| 9 итерация  | 0,004       | 0,002       | 9,574       | 0,249                    | 0,012      |
| 10 итерация | 0,004       | 0,002       | 13,399      | 0,246                    | 0,012      |
| Среднее     | 0,00540     | 0,00220     | 10,81570    | 0,25930                  | 0,01590    |
|             |             |             |             |                          |            |
| PostgreSQL  |             |             |             |                          |            |
|             | Первый тест | Второй тест | Третий тест | Четвертый тест           | Пятый тест |
| 1 итерация  | 0,009       | 0,049       | 77,509      | 7,878                    | 0,004      |
| 2 итерация  | 0,006       | 0,037       | 81,844      | 5,668                    | 0,008      |
| 3 итерация  | 0,006       | 0,038       | 74,859      | 5,573                    | 0,001      |
| 4 итерация  | 0,005       | 0,038       | 75,937      | 5,560                    | 0,002      |
| 5 итерация  | 0,005       | 0,039       | 75,084      | 5,574                    | 0,001      |
| 6 итерация  | 0,009       | 0,039       | 75,146      | 5,556                    | 0,003      |
| 7 итерация  | 0,005       | 0,038       | 75,497      | 5,547                    | 0,002      |
| 8 итерация  | 0,005       | 0,039       | 75,054      | 5,559                    | 0,001      |
| 9 итерация  | 0,006       | 0,039       | 76,222      | 5,588                    | 0,001      |
| 10 итерация | 0,006       | 0,039       | 77,241      | 5,565                    | 0,001      |
| Среднее     | 0,00620     | 0,03950     | 76,43930    | 5,80680                  | 0,00240    |

На размере шума в 100 миллионов записей обе СУБД показали непозволительное время чтения данных с оценками пользователя, таким образом, для хранения оценок/закладок/рецензий было решено использовать кластер MongoDB с тремя шардами.

## Проектирование архитектуры

Следующей целью было нарисовать текущую архитектуру получившейся системы и показать все основные взаимодействия между микросервисами и базами данных.

Используя библиотекой diagrams для python мы нарисовали диаграму текущего состояния системы (**AS-IS**) и схему того, что нужно изменить или добавить в вашей системе (схема **TO-BE**). Решено было сохранять сообщения о событиях UGC через fastapi gateway в Kafka и Redis, а дале компонентом etl переносить накопившиеся данные в аналитическое хранилище.

**Схема системы AS-IS**

<img src="https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-as-is-architecture.png" width="300">

**Схема системы TO-BE**

<img src="https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-to-be-architecture.png" width="300">

**Декомпозиция сервиса**

<img src="https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-ugc-decomposition.png" width="300">

**Декомпозиция компонента etl**

<img src="https://github.com/DmitriiBogomolov/Cinema-Team-Project/blob/main/architecture/sprint-8-elt-decomposition.png" width="300">

## Доступные команды:

### Поднять все (клилксаус, кафка, монго, редис, приложение, etl)

        make all

### Поднять облегченную версию без клилксауса и etl

        make run_service

### Запустить только базы данных-зависимости приложения (кафка, монго, редис)

        make run_dbs

### Поднять необходимые зависимости и запустить функциональные тесты

        make run_test_app

### Остановить все

        make clean

### Запустить приложение локально

        uvicorn app.main:app --reload
